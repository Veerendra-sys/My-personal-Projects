# -*- coding: utf-8 -*-
"""Chatbot-Driven Universities NLP Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i2FoQfwy45nN7geLjPVdIn0X6CNmwuva
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from textblob import TextBlob
from wordcloud import WordCloud

warnings.filterwarnings('ignore')

try:
    # Try reading with the original encoding
    df = pd.read_csv("/content/AI_Chatbots_Students_Attitude_Dataset_EN.csv", encoding='latin-1')
except pd.errors.ParserError:
    # If it fails, try reading with a different delimiter and error handling
    df = pd.read_csv("/content/AI_Chatbots_Students_Attitude_Dataset_EN.csv", encoding='latin-1', delimiter=';', on_bad_lines='skip')  # or try delimiter='\t' for tab-separated files
df

"""#1. Sentiment Analysis of Responses
###Analyze the sentiment of responses for each question and visualize the sentiment distribution.



"""

# Combine responses for sentiment analysis
responses = df[['Q1', 'Q2', 'Q3', 'Q4', 'Q5.1', 'Q5.2', 'Q5.3', 'Q5.4', 'Q5.5', 'Q8.2', 'Q8.3', 'Q8.4', 'Q8.5', 'Q9.1', 'Q9.2', 'Q9.3', 'Q9.4', 'Q9.5', 'Q10']].fillna('')

# Function to calculate sentiment
def get_sentiment(text):
    analysis = TextBlob(text)
    return analysis.sentiment.polarity

# Calculate sentiment for each response
sentiments = responses.apply(lambda x: x.apply(get_sentiment).mean(), axis=1)

# Plot sentiment distribution
plt.figure(figsize=(14, 6))
plt.hist(sentiments, bins=20, edgecolor='k', alpha=0.7)
plt.title('Sentiment Distribution of Responses')
plt.xlabel('Sentiment Score')
plt.ylabel('Frequency')
#plt.grid(True)
plt.show()

"""The histogram represents the Sentiment Distribution of Responses based on sentiment scores. The x-axis denotes the sentiment score, while the y-axis represents the frequency of responses with corresponding sentiment values.

Combines all responses into a single text per entry, calculated the sentiment score using TextBlob, and visualizes the distribution of sentiment scores. The histogram shows how positive or negative the responses are on average.

Combines all responses into a single text per entry, calculated the sentiment score using TextBlob, and visualizes the distribution of sentiment scores. The histogram shows how positive or negative the responses are on average.
"""

import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Combine all responses into one large text
text = ' '.join(df[['Q1', 'Q2', 'Q3', 'Q4', 'Q5.1', 'Q5.2', 'Q5.3', 'Q5.4',
                    'Q5.5', 'Q8.2', 'Q8.3', 'Q8.4', 'Q8.5', 'Q9.1', 'Q9.2',
                    'Q9.3', 'Q9.4', 'Q9.5', 'Q10']].fillna('').astype(str).agg(' '.join, axis=1))

# Generate the word cloud with an improved theme
wordcloud = WordCloud(width=1000, height=500, background_color='black',
                      colormap='viridis', contour_color='steelblue',
                      contour_width=2, max_words=200).generate(text)

# Plot the word cloud
plt.figure(figsize=(14, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Responses', fontsize=18, fontweight='bold', color='white')
plt.gca().patch.set_facecolor('black')  # Set background for the plot
plt.show()

"""Identify topics in the responses using LDA and visualize the most important words for each topic.

"""

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import seaborn as sns

# Combine all responses into one large text
text = df[['Q1', 'Q2', 'Q3', 'Q4', 'Q5.1', 'Q5.2', 'Q5.3', 'Q5.4', 'Q5.5', 'Q8.2', 'Q8.3', 'Q8.4', 'Q8.5', 'Q9.1', 'Q9.2', 'Q9.3', 'Q9.4', 'Q9.5', 'Q10']].fillna('').astype(str).agg(' '.join, axis=1)

# Vectorize the text data
vectorizer = CountVectorizer(stop_words='english')
X = vectorizer.fit_transform(text)

# Apply LDA
lda = LatentDirichletAllocation(n_components=5, random_state=0)
lda.fit(X)

# Display topics
def display_topics(model, feature_names, no_top_words):
    for topic_idx, topic in enumerate(model.components_):
        print(f"Topic #{topic_idx}:")
        print(" ".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))
    print()

no_top_words = 10
display_topics(lda, vectorizer.get_feature_names_out(), no_top_words)

# Visualize the topics
topic_words = []
for topic_idx, topic in enumerate(lda.components_):
    words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-no_top_words - 1:-1]]
    topic_words.append(words)

# Create a DataFrame for visualization
topics_df = pd.DataFrame(topic_words, columns=[f'Word {i+1}' for i in range(no_top_words)])
topics_df.index.name = 'Topic'
topics_df = topics_df.reset_index()

# Define the number of words to display
no_top_words = 10

# Extract words and their corresponding topic weight (importance)
topic_word_weights = []
for topic_idx, topic in enumerate(lda.components_):
    words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-no_top_words - 1:-1]]
    weights = [topic[i] for i in topic.argsort()[:-no_top_words - 1:-1]]
    topic_word_weights.append(pd.DataFrame({'Words': words, 'Weights': weights}))

# Plot top words for each topic
for i, topic_df in enumerate(topic_word_weights):
    plt.figure(figsize=(12, 4))
    sns.barplot(x='Weights', y='Words', data=topic_df, palette='viridis')
    plt.title(f'Topic {i+1} - Top Words')
    plt.xlabel('Weight')
    plt.ylabel('Word')
    plt.show()

"""Analyze the length of responses to each question and visualize the distribution.

"""

# Calculate length of responses for each question
response_lengths = df[['Q1', 'Q2', 'Q3', 'Q4', 'Q5.1', 'Q5.2', 'Q5.3', 'Q5.4', 'Q5.5', 'Q8.2', 'Q8.3', 'Q8.4', 'Q8.5', 'Q9.1', 'Q9.2', 'Q9.3', 'Q9.4', 'Q9.5', 'Q10']].fillna('').applymap(len)

# Plot response length distribution for each question
plt.figure(figsize=(14, 8))
for col in response_lengths.columns:
    plt.hist(response_lengths[col], bins=20, alpha=0.5, label=col)

plt.title('Distribution of Response Lengths for Each Question')
plt.xlabel('Response Length')
plt.ylabel('Frequency')
plt.legend(loc='upper right')
plt.grid(True)
plt.show()

"""This analysis helps in understanding how respondents interacted with different questions. If more detailed responses were expected, revising the survey design (e.g., encouraging elaboration) might be necessary."""