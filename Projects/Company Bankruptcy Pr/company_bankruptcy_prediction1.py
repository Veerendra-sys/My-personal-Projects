# -*- coding: utf-8 -*-
"""Company bankruptcy prediction1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i5W9eNnAyq-shFGinKxWhnVMV1WZ029w
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression, LassoCV
from sklearn.feature_selection import mutual_info_classif
from statsmodels.api import OLS, add_constant
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.preprocessing import StandardScaler
import imblearn
from imblearn.under_sampling import NearMiss
from collections import Counter
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, recall_score, precision_score, roc_auc_score
from sklearn.model_selection import GridSearchCV
from imblearn.over_sampling import SMOTE

"""Cleaning the Data: The data was checked for null values, categoricalvalues and primary inspection was performed."""

df=pd.read_csv('/content/data.csv')

df

df.head(5)

df.isnull().sum()

df.tail()

df.columns()

df.info

df.isnull().sum().sum()
#This conculdes there is no null values in the aboue data set

"""Feature Selection
Techniques such as VIF, p-value, L-1 Regularization and Information Gain were performed to select important features.
"""

# Rename columns to remove extra spaces
df.columns = df.columns.str.strip()

# Separate features and target variable
X = df.drop(columns=["Bankrupt?"])  # Predictors
y = df["Bankrupt?"]  # Target

# Step 1: Variance Inflation Factor (VIF) to remove multicollinearity
def calculate_vif(X):
    X = add_constant(X)  # Add constant for OLS regression
    vif_data = pd.DataFrame()
    vif_data["Feature"] = X.columns
    vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
    return vif_data[vif_data["VIF"] < 10]  # Keep features with VIF < 10

vif_filtered_features = calculate_vif(X)
X_vif_selected = X[vif_filtered_features["Feature"].tolist()]  # Select only low VIF features

# Step 2: p-value Analysis (Statistical Significance Test)
X_with_const = add_constant(X_vif_selected)  # Add constant for OLS
model = OLS(y, X_with_const).fit()
p_values = model.pvalues
p_selected_features = p_values[p_values < 0.05].index.tolist()  # Select features with p-value < 0.05
X_p_selected = X_vif_selected[p_selected_features]  # Select features

# Step 3: L1 Regularization (Lasso) for Feature Selection
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_p_selected)  # Scale features for Lasso
lasso = LassoCV(cv=5, random_state=42).fit(X_scaled, y)  # Lasso with cross-validation
selected_features_lasso = X_p_selected.columns[lasso.coef_ != 0].tolist()  # Keep non-zero coefficient features
X_lasso_selected = X_p_selected[selected_features_lasso]

# Step 4: Information Gain (Mutual Information) for Feature Ranking
info_gain = mutual_info_classif(X_lasso_selected, y, random_state=42)
info_gain_df = pd.DataFrame({"Feature": X_lasso_selected.columns, "InfoGain": info_gain})
info_gain_df = info_gain_df.sort_values(by="InfoGain", ascending=False)  # Sort by importance
selected_features_final = info_gain_df["Feature"].tolist()[:20]  # Top 20 features

# Final Feature Set
X_final = X_lasso_selected[selected_features_final]
print(f"Final Selected Features ({len(selected_features_final)}):\n", selected_features_final)

# Save selected features for further modeling
X_final.to_csv("selected_features.csv", index=False)
y.to_csv("target_variable.csv", index=False)

print("Feature Selection Complete! ðŸš€ DONE BY Veerendra Amaravathi")

